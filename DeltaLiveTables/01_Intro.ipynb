{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58b2c646-f2d7-4bbf-98b2-8e13666e9c4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Streaming table**\t- Each record is processed exactly once. This assumes an append-only source.\n",
    "- **Materialized view** - Records are processed as required to return accurate results for the current data state. Materialized views should be used for data processing tasks such as transformations, aggregations, or pre-computing slow queries and frequently used computations.\n",
    "- **View** - Records are processed each time the view is queried. Use views for intermediate transformations and data quality checks that should not be published to public datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a7d0c111-263d-4996-839b-29ec813e8eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b54a5d-bb82-4ffb-94db-81254b4d73bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a streaming table for Orders\n",
    "@dlt.table(\n",
    "  table_properties = { \"quality\": \"bronze\" },\n",
    "  comment = \"Order bronze table\"\n",
    ")\n",
    "\n",
    "def orders_bronze():\n",
    "  df = spark.readStream.table(\"dev.bronze.orders_raw\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c03ba084-d2b9-413c-a78b-ccef9d8cf097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a materialized view for customers\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\": \"bronze\"},\n",
    "  comment = \"Customer bronze table\",\n",
    "  name = \"customer_bronze\"\n",
    ")\n",
    "\n",
    "def cust_bronze():\n",
    "  df = spark.read.table(\"dev.bronze.customer_raw\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "156874d2-8ada-4f7f-a42e-051bdbd8c6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a view to join orders with customers\n",
    "@dlt.table(\n",
    "  comment = \"Joined View\"\n",
    ")\n",
    "\n",
    "def joined_vw():\n",
    "  df_c = spark.read.table(\"LIVE.customer_bronze\")\n",
    "  df_o = spark.read.table(\"LIVE.orders_bronze\")\n",
    "  df_join = df_o.join(df_c, how = \"left_outer\", on=df_c.c_custkey==df_o.o_custkey)\n",
    "  \n",
    "  return df_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa782dde-2602-4a97-a780-813605c32158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create MV to add new column\n",
    "from pyspark.sql.functions import current_timestamp, count, sum\n",
    "\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\": \"silver\"},\n",
    "  comment = \"Joined Table\",\n",
    "  name = \"joined_silver\"\n",
    ")\n",
    "\n",
    "def joined_silver():\n",
    "  df = spark.read.table(\"LIVE.joined_vw\").withColumn(\"__insert_date\", current_timestamp())\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7481ae72-a81f-40c9-8828-765135db35bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate based on c_mktsegment and find the count of oder (c_orderkey)\n",
    "@dlt.table(\n",
    "  table_properties = { \"quality\": \"gold\" },\n",
    "  comment = \"Orders aggregated table\"\n",
    ")\n",
    "\n",
    "def orders_agg_gold():\n",
    "  df = spark.read.table(\"LIVE.joined_silver\")\n",
    "\n",
    "  df_final = df.groupBy(\"c_mktsegment\").agg(count(\"o_orderkey\").alias(\"count_orders\"), sum(\"o_totalprice\").alias(\"sum_totalprice\")).withColumn(\"__insert_date\", current_timestamp())\n",
    "\n",
    "  return df_final"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5511691943590393,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_Intro",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
