{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58b2c646-f2d7-4bbf-98b2-8e13666e9c4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- **Streaming table**\t- Each record is processed exactly once. This assumes an append-only source.\n",
    "- **Materialized view** - Records are processed as required to return accurate results for the current data state. Materialized views should be used for data processing tasks such as transformations, aggregations, or pre-computing slow queries and frequently used computations.\n",
    "- **View** - Records are processed each time the view is queried. Use views for intermediate transformations and data quality checks that should not be published to public datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "a7d0c111-263d-4996-839b-29ec813e8eea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "62e0bbd6-72a6-4e2c-8bfa-3580febc9210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "_order_status = spark.conf.get(\"custom.orderStatus\", \"NA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9b54a5d-bb82-4ffb-94db-81254b4d73bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a streaming table for Orders\n",
    "@dlt.table(\n",
    "  table_properties = { \"quality\": \"bronze\" },\n",
    "  comment = \"Order bronze table\"\n",
    ")\n",
    "\n",
    "def orders_bronze():\n",
    "  df = spark.readStream.table(\"dev.bronze.orders_raw\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d8e6130c-ecf5-468e-8d88-c3674531c11c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a streaming table for Orders Autoloader\n",
    "@dlt.table(\n",
    "  table_properties = { \"quality\": \"bronze\" },\n",
    "  comment = \"Order Autoloader\",\n",
    "  name = \"orders_autoloader_bronze\"\n",
    ")\n",
    "\n",
    "def func():\n",
    "  df = (\n",
    "      spark\n",
    "      .readStream\n",
    "      .format('cloudFiles')\n",
    "      .option('cloudFiles.schemaHints',\"o_orderkey long, o_custkey long, o_orderstatus string, o_totalprice decimal(18,2),o_orderdate date, o_orderpriority string, o_clerk string, o_shippriority integer, o_comment string\")\n",
    "      .option(\"cloudFiles.schemaLocation\",\"/Volumes/dev/etl/landing/autoloader/schemas/1\")\n",
    "      .option(\"cloudFiles.format\",\"CSV\")\n",
    "      .option(\"pathGlobfilter\",\"*.csv\")\n",
    "      .option(\"cloudFiles.schemaEvolutionMode\",\"none\")\n",
    "      .load(\"/Volumes/dev/etl/landing/files/\")\n",
    "  )\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5948dba1-4465-4852-9e1f-1778d775dfd5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create streaming table with union of two orders tables\n",
    "\n",
    "dlt.create_streaming_table(\"orders_union_bronze\")\n",
    "\n",
    "# Appendflow Stream table\n",
    "@dlt.append_flow(\n",
    "    target = \"orders_union_bronze\"\n",
    ")\n",
    "def order_delta_append():\n",
    "    df = spark.readStream.table(\"LIVE.orders_bronze\")\n",
    "    return df\n",
    "\n",
    "# Appendflow Autoloader\n",
    "@dlt.append_flow(\n",
    "    target = \"orders_union_bronze\"\n",
    ")\n",
    "def order_autoloader_append():\n",
    "    df = spark.readStream.table(\"LIVE.orders_autoloader_bronze\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c03ba084-d2b9-413c-a78b-ccef9d8cf097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # Create a materialized view for customers\n",
    "# @dlt.table(\n",
    "#   table_properties = {\"quality\": \"bronze\"},\n",
    "#   comment = \"Customer bronze table\",\n",
    "#   name = \"customer_bronze\"\n",
    "# )\n",
    "\n",
    "# def cust_bronze():\n",
    "#   df = spark.read.table(\"dev.bronze.customer_raw\")\n",
    "#   return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "daa3a0dd-9171-42ed-ab18-34903f1fd0fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a materialized view for customers\n",
    "@dlt.view(\n",
    "  comment = \"Customer bronze view\",\n",
    ")\n",
    "\n",
    "def customer_bronze_vw():\n",
    "  df = spark.readStream.table(\"dev.bronze.customer_raw\")\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11ba638b-0f3c-415c-a954-bbc413a25436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "# Write SCD Type 1 table - \n",
    "dlt.create_streaming_table(\"customer_scd1_bronze\")\n",
    "\n",
    "dlt.apply_changes(\n",
    "    target = \"customer_scd1_bronze\",\n",
    "    source = \"customer_bronze_vw\",\n",
    "    keys = [\"c_custkey\"],\n",
    "    stored_as_scd_type = 1,\n",
    "    apply_as_deletes = expr(\"__src_action = 'D'\"),\n",
    "    apply_as_truncates = expr(\"__src_action = 'T'\"),\n",
    "    sequence_by = \"__src_insert_dt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90c1d6af-a26d-41db-ac60-2cc194cd29b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "# Write SCD Type 2 table - \n",
    "dlt.create_streaming_table(\"customer_scd2_bronze\")\n",
    "\n",
    "dlt.apply_changes(\n",
    "    target = \"customer_scd2_bronze\",\n",
    "    source = \"customer_bronze_vw\",\n",
    "    keys = [\"c_custkey\"],\n",
    "    stored_as_scd_type = 2,\n",
    "    except_column_list=[\"__src_action\",\"__src_insert_dt\"],\n",
    "    sequence_by = \"__src_insert_dt\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "156874d2-8ada-4f7f-a42e-051bdbd8c6ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create a view to join orders with customers\n",
    "@dlt.table(\n",
    "  comment = \"Joined View\"\n",
    ")\n",
    "\n",
    "def joined_vw():\n",
    "  df_c = spark.read.table(\"LIVE.customer_scd2_bronze\").where(\"__END_AT is null\")\n",
    "  df_o = spark.read.table(\"LIVE.orders_union_bronze\")\n",
    "  df_join = df_o.join(df_c, how = \"left_outer\", on=df_c.c_custkey==df_o.o_custkey)\n",
    "  \n",
    "  return df_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fa782dde-2602-4a97-a780-813605c32158",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create MV to add new column\n",
    "from pyspark.sql.functions import current_timestamp, count, sum\n",
    "\n",
    "@dlt.table(\n",
    "  table_properties = {\"quality\": \"silver\"},\n",
    "  comment = \"Joined Table\",\n",
    "  name = \"joined_silver\"\n",
    ")\n",
    "\n",
    "def joined_silver():\n",
    "  df = spark.read.table(\"LIVE.joined_vw\").withColumn(\"__insert_date\", current_timestamp())\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7481ae72-a81f-40c9-8828-765135db35bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Aggregate based on c_mktsegment and find the count of oder (c_orderkey)\n",
    "@dlt.table(\n",
    "  table_properties = { \"quality\": \"gold\" },\n",
    "  comment = \"Orders aggregated table\"\n",
    ")\n",
    "\n",
    "def orders_agg_gold():\n",
    "  df = spark.read.table(\"LIVE.joined_silver\")\n",
    "\n",
    "  df_final = df.groupBy(\"c_mktsegment\").agg(count(\"o_orderkey\").alias(\"count_orders\"), sum(\"o_totalprice\").alias(\"sum_totalprice\")).withColumn(\"__insert_date\", current_timestamp())\n",
    "\n",
    "  return df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "208e80fa-771f-4da9-a75f-b6ab771c6a96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for _status in _order_status.split(\",\"): \n",
    "    # Aggregate based on c_mktsegment and find the count of oder (c_orderkey)\n",
    "    @dlt.table(\n",
    "        table_properties = { \"quality\": \"gold\" },\n",
    "        comment = \"Orders aggregated table\",\n",
    "        name = f\"orders_agg_{_status}_gold\"\n",
    "    )\n",
    "\n",
    "    def func():\n",
    "        df = spark.read.table(\"LIVE.joined_silver\")\n",
    "\n",
    "        df_final = df.where(f\"o_orderstatus='{_status}'\").groupBy(\"c_mktsegment\").agg(count(\"o_orderkey\").alias(\"count_orders\"), sum(\"o_totalprice\").alias(\"sum_totalprice\")).withColumn(\"__insert_date\", current_timestamp())\n",
    "\n",
    "        return df_final"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5511691943590393,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone_1) 01_Intro",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
