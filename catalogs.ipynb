{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9a3cc52-daf0-4841-985b-ceeaac985b59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE CATALOG EXTENDED DEV;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff5ca6c2-bc32-405e-a466-d6fc6176d28f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### CREATE CATALOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc97f6da-b6da-4f9e-a18c-0a45e3618125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE CATALOG dev_sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e92eb015-9f58-49a4-9095-6fdf69e33a27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DROP CATALOG dev_sql CASCADE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a418b2e-c148-441c-96b7-8943260633a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Catalog w/ External Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "516e4a15-e8bb-47eb-bc79-dc1462b452b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "-- External Location\n",
    "\n",
    "CREATE EXTERNAL LOCATION ext_catalog\n",
    "  URL 'abfss://data@adbstorageuks.dfs.core.windows.net/adb/catalog'\n",
    "  WITH (STORAGE CREDENTIAL `sc_catalog_storage`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7051d9f4-745f-4ceb-8b44-478674335c6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "  CREATE CATALOG dev_ext\n",
    "  MANAGED LOCATION 'abfss://data@adbstorageuks.dfs.core.windows.net/adb/catalog'\n",
    "  COMMENT 'This is an external storage catalog'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "678f9841-c974-47ad-817e-b80b0ad3f176",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DESCRIBE CATALOG EXTENDED dev_ext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db87cc32-6888-4962-bbbf-3afc5ad8c027",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### CREATE SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c5fc402-8c72-4280-8af3-dd0ca06574aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Create schema bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dded3835-95ee-43b4-8f07-a0c3068d53e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "CREATE TABLE bronze.emp (\n",
    "  emp_id int,\n",
    "  emp_name string,\n",
    "  dept_code string\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c65547cf-74e8-4938-8b72-7b10aa6549a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "describe extended bronze.emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e27f283b-770d-40d3-bd18-acb1f4e55262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "INSERT INTO bronze.emp (emp_id, emp_name, dept_code) VALUES (1, 'John Doe', 'D001');\n",
    "INSERT INTO bronze.emp (emp_id, emp_name, dept_code) VALUES (2, 'Jane Smith', 'D002');\n",
    "INSERT INTO bronze.emp (emp_id, emp_name, dept_code) VALUES (3, 'Alice Johnson', 'D003');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66f119f4-d6a3-4c9e-9033-cbdebb00438c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "INSERT INTO bronze.emp (emp_id, emp_name, dept_code) VALUES (4, 'Chris Johnson', 'D002');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a0327d7-f379-44bf-89da-e7519d103e9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "select * from bronze.emp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1fedb5d1-2f6e-409c-9946-1356e5614548",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "\n",
    "dbutils.fs.ls('dbfs:/user/hive/warehouse/bronze.db/emp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9fe96e7-d51d-4db8-95c5-7ef3d7b6c0ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "show catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "239b06bb-1470-459b-b725-83bdf34af216",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    " display(dbutils.fs.ls('/databricks-datasets'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0716021a-1e4e-44f0-8bbe-5d6b22d99b4d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "dbutils.fs.head('dbfs:/databricks-datasets/SPARK_README.md', 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "661a77ef-1bd7-4d73-97a2-5546787dfa42",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "'# Apache Spark\\n\\nSpark is a fast and general cluster computing system for Big Data. It provides\\nhigh-level APIs in Scala, Java, Python, and R, and an optimized engine that\\nsupports general computation graphs for data analysis. It also supports a\\nrich set of higher-level tools including Spark SQL for SQL and DataFrames,\\nMLlib for machine learning, GraphX for graph processing,\\nand Spark Streaming for stream processing.\\n\\n<http://spark.apache.org/>\\n\\n\\n## Online Documentation\\n\\nYou can find the latest Spark documentation, including a programming\\nguide, on the [project web page](http://spark.apache.org/documentation.html)\\nand [project wiki](https://cwiki.apache.org/confluence/display/SPARK).\\nThis README file only contains basic setup instructions.\\n\\n## Building Spark\\n\\nSpark is built using [Apache Maven](http://maven.apache.org/).\\nTo build Spark and its example programs, run:\\n\\n    build/mvn -DskipTests clean package\\n\\n(You do not need to do this if you downloaded a pre-built package.)\\nMore detailed documentation is available from the project site, at\\n[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).\\n\\n## Interactive Scala Shell\\n\\nThe easiest way to start using Spark is through the Scala shell:\\n\\n    ./bin/spark-shell\\n\\nTry the following command, which should return 1000:\\n\\n    scala> sc.parallelize(1 to 1000).count()\\n\\n## Interactive Python Shell\\n\\nAlternatively, if you prefer Python, you can use the Python shell:\\n\\n    ./bin/pyspark\\n\\nAnd run the following command, which should also return 1000:\\n\\n    >>> sc.parallelize(range(1000)).count()\\n\\n## Example Programs\\n\\nSpark also comes with several sample programs in the `examples` directory.\\nTo run one of them, use `./bin/run-example <class> [params]`. For example:\\n\\n    ./bin/run-example SparkPi\\n\\nwill run the Pi example locally.\\n\\nYou can set the MASTER environment variable when running examples to submit\\nexamples to a cluster. This can be a mesos:// or spark:// URL,\\n\"yarn\" to run on YARN, and \"local\" to run\\nlocally with one thread, or \"local[N]\" to run locally with N threads. You\\ncan also use an abbreviated class name if the class is in the `examples`\\npackage. For instance:\\n\\n    MASTER=spark://host:7077 ./bin/run-example SparkPi\\n\\nMany of the example programs print usage help if no params are given.\\n\\n## Running Tests\\n\\nTesting first requires [building Spark](#building-spark). Once Spark is built, tests\\ncan be run using:\\n\\n    ./dev/run-tests\\n\\nPlease see the guidance on how to\\n[run tests for a module, or individual tests](https://cwiki.apache.org/confluence/display/SPARK/Useful+Developer+Tools).\\n\\n## A Note About Hadoop Versions\\n\\nSpark uses the Hadoop core library to talk to HDFS and other Hadoop-supported\\nstorage systems. Because the protocols have changed in different versions of\\nHadoop, you must build Spark against the same version that your cluster runs.\\n\\nPlease refer to the build documentation at\\n[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)\\nfor detailed guidance on building for a particular distribution of Hadoop, including\\nbuilding for particular Hive and Hive Thriftserver distributions.\\n\\n## Configuration\\n\\nPlease refer to the [Configuration Guide](http://spark.apache.org/docs/latest/configuration.html)\\nin the online documentation for an overview on how to configure Spark.\\n'"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "catalogs",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
